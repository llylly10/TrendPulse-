import sqlite3
import os
import json
import tiktoken
from openai import OpenAI
from dotenv import load_dotenv
import pandas as pd
import numpy as np

# =========================
# 1. åˆå§‹åŒ– & é…ç½®
# =========================

load_dotenv()

DB_NAME = "multi_source.db"
MODEL = "gpt-5.2"  # ä½¿ç”¨ä½ ä»¬æä¾›çš„æ¨¡å‹

MAX_TOKENS_PER_BATCH = 4000
SAMPLE_SIZE = 100

# âœ… å…³é”®ä¿®å¤ï¼šæ‰‹åŠ¨æŒ‡å®š tokenizerï¼ˆä¸æ¨¡å‹åè§£è€¦ï¼‰
ENCODING = tiktoken.get_encoding("cl100k_base")

client = OpenAI(
    api_key=os.getenv("OPENAI_API_KEY"),
    base_url=os.getenv("OPENAI_BASE_URL")
)

# =========================
# 2. å·¥å…·å‡½æ•°
# =========================

def get_token_count(text: str) -> int:
    """å®‰å…¨è®¡ç®— token æ•°é‡ï¼ˆä¸ä¾èµ–æ¨¡å‹åï¼‰"""
    return len(ENCODING.encode(text))


def filter_dirty_data(df: pd.DataFrame) -> pd.DataFrame:
    """åˆæ­¥è¿‡æ»¤è„æ•°æ®"""
    initial_count = len(df)

    df = df[df["content"].notna()].copy()
    df = df[df["content"].str.len() > 10].copy()

    ad_keywords = [
        "åŠ å¾®ä¿¡", "è”ç³»æ–¹å¼", "åˆ·å•", "å…¼èŒ",
        "ä¼˜æƒ åˆ¸", "ç‚¹æˆ‘é¢†å–", "vx", "vxï¼š"
    ]
    for kw in ad_keywords:
        df = df[~df["content"].str.contains(kw, na=False)]

    print(f"ğŸ§¹ è„æ•°æ®è¿‡æ»¤å®Œæˆ: {initial_count} -> {len(df)}")
    return df


# =========================
# 3. Map é˜¶æ®µ
# =========================

def map_phase(batches: list[str], language: str = "zh") -> list[dict]:
    map_results = []

    for i, batch in enumerate(batches):
        print(f"ğŸ§  æ­£åœ¨å¤„ç†ç¬¬ {i+1}/{len(batches)} ä¸ªæ‰¹æ¬¡...")

        if language == "en":
            prompt = f"""
You are a professional data analyst. Please analyze the following batch of social media comments.

Task:
1. Give an overall sentiment score (0-100)
2. Extract key points or controversies (max 5 items)
3. Determine if it contains obvious spam

Text to analyze:
\"\"\"
{batch}
\"\"\"

Please return ONLY valid JSON:
{{
  "sentiment_score": 75,
  "key_points": ["Point 1", "Point 2"],
  "spam_info": "None"
}}
"""
        else:
            prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ•°æ®åˆ†æå¸ˆï¼Œè¯·åˆ†æä»¥ä¸‹ç¤¾äº¤åª’ä½“è¯„è®ºæ‰¹æ¬¡ã€‚

ä»»åŠ¡ï¼š
1. ç»™å‡ºæ•´ä½“æƒ…æ„Ÿå¾—åˆ†ï¼ˆ0-100ï¼‰
2. æå–æ ¸å¿ƒè§‚ç‚¹æˆ–äº‰è®®ç‚¹ï¼ˆæœ€å¤š 5 æ¡ï¼‰
3. åˆ¤æ–­æ˜¯å¦ä»åŒ…å«æ˜æ˜¾åƒåœ¾ä¿¡æ¯

å¾…åˆ†ææ–‡æœ¬ï¼š
\"\"\"
{batch}
\"\"\"

è¯·ä»…è¿”å›åˆæ³• JSONï¼š
{{
  "sentiment_score": 75,
  "key_points": ["è§‚ç‚¹1", "è§‚ç‚¹2"],
  "spam_info": "æ— "
}}
"""

        try:
            response = client.chat.completions.create(
                model=MODEL,
                messages=[
                    {"role": "system", "content": "You are a professional data analysis assistant." if language == "en" else "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ•°æ®åˆ†æåŠ©æ‰‹ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                response_format={"type": "json_object"}
            )

            result = json.loads(response.choices[0].message.content)
            map_results.append(result)

        except Exception as e:
            print(f"âŒ æ‰¹æ¬¡ {i+1} å¤„ç†å¤±è´¥: {e}")

    return map_results


# =========================
# 4. Reduce é˜¶æ®µ
# =========================

def reduce_phase(map_results: list[dict], language: str = "zh", keyword: str = None) -> dict | None:
    print(f"ğŸ”„ æ­£åœ¨æ±‡æ€»æœ€ç»ˆåˆ†æç»“æœ (å…³é”®è¯: {keyword or 'æœªæŒ‡å®š'})...")

    all_scores = []
    all_points = []

    for r in map_results:
        all_scores.append(r.get("sentiment_score", 50))
        all_points.extend(r.get("key_points", []))

    avg_sentiment = round(float(np.mean(all_scores)), 2)
    points_text = "\n".join(f"- {p}" for p in all_points)
    
    # ç¡®å®šä¸»é¢˜åç§°
    topic_name = keyword if keyword else "ä¸»é¢˜"

    if language == "en":
        prompt = f"""
You are a senior public opinion expert. Please complete the final summary based on the following points about "{topic_name}".

List of points:
\"\"\"
{points_text}
\"\"\"

Tasks:
1. Summarize 3 main controversies about {topic_name}
2. Generate a 150-200 word summary about {topic_name}
3. Generate a simple Mermaid.js mindmap (graph TD) with max 8 nodes, using "{topic_name}" as the root node
4. Label sentiment for each node

IMPORTANT for mermaid_graph:
- Root node MUST be: A[{topic_name}]
- Use simple node names (max 10 characters per node)
- Maximum 8 nodes total
- Use format: graph TD; A[{topic_name}] --> B[Point1]; A --> C[Point2]; B --> D[Detail];
- Keep it simple and clear

Node sentiment labeling:
- Judge sentiment for each node (except main topic)
- Return format: {{"NodeID": "positive/neutral/negative"}}
- Example: {{"B": "positive", "C": "negative", "D": "neutral"}}

Return ONLY valid JSON:
{{
  "final_controversies": ["Controversy 1", "Controversy 2", "Controversy 3"],
  "human_summary": "Summary content about {topic_name}",
  "mermaid_graph": "graph TD; A[{topic_name}] --> B[Point1]; A --> C[Point2];",
  "node_sentiments": {{"B": "positive", "C": "negative"}}
}}
"""
    else:
        prompt = f"""
ä½ æ˜¯é«˜çº§èˆ†æƒ…åˆ†æä¸“å®¶ï¼Œè¯·åŸºäºä»¥ä¸‹å…³äº"{topic_name}"çš„è§‚ç‚¹å®Œæˆæœ€ç»ˆæ±‡æ€»ã€‚

è§‚ç‚¹åˆ—è¡¨ï¼š
\"\"\"
{points_text}
\"\"\"

ä»»åŠ¡ï¼š
1. æ€»ç»“ 3 ä¸ªå…³äº {topic_name} çš„ä¸»è¦äº‰è®®ç‚¹
2. ç”Ÿæˆä¸€æ®µ 150-200 å­—å…³äº {topic_name} çš„é€šä¿—æ‘˜è¦
3. ç”Ÿæˆä¸€ä¸ªç®€æ´çš„ Mermaid.js æ€ç»´å¯¼å›¾ï¼ˆgraph TDï¼‰ï¼Œæœ€å¤š 8 ä¸ªèŠ‚ç‚¹ï¼Œä»¥"{topic_name}"ä½œä¸ºæ ¹èŠ‚ç‚¹
4. ä¸ºæ¯ä¸ªèŠ‚ç‚¹æ ‡æ³¨æƒ…æ„Ÿå€¾å‘

æ€ç»´å¯¼å›¾è¦æ±‚ï¼š
- æ ¹èŠ‚ç‚¹å¿…é¡»æ˜¯ï¼šA[{topic_name}]
- èŠ‚ç‚¹åç§°ç®€çŸ­ï¼ˆæ¯ä¸ªèŠ‚ç‚¹æœ€å¤š 6 ä¸ªæ±‰å­—ï¼‰
- æ€»å…±æœ€å¤š 8 ä¸ªèŠ‚ç‚¹
- æ ¼å¼ï¼šgraph TD; A[{topic_name}] --> B[è§‚ç‚¹1]; A --> C[è§‚ç‚¹2]; B --> D[ç»†èŠ‚];
- ä¿æŒç®€æ´æ¸…æ™°

èŠ‚ç‚¹æƒ…æ„Ÿæ ‡æ³¨ï¼š
- ä¸ºæ¯ä¸ªèŠ‚ç‚¹ï¼ˆé™¤äº†ä¸»é¢˜èŠ‚ç‚¹ï¼‰åˆ¤æ–­æƒ…æ„Ÿå€¾å‘
- è¿”å›æ ¼å¼ï¼š{{"èŠ‚ç‚¹ID": "positive/neutral/negative"}}
- ä¾‹å¦‚ï¼š{{"B": "positive", "C": "negative", "D": "neutral"}}

ä»…è¿”å›åˆæ³• JSONï¼š
{{
  "final_controversies": ["äº‰è®®ç‚¹1", "äº‰è®®ç‚¹2", "äº‰è®®ç‚¹3"],
  "human_summary": "å…³äº{topic_name}çš„æ‘˜è¦å†…å®¹",
  "mermaid_graph": "graph TD; A[{topic_name}] --> B[è§‚ç‚¹1]; A --> C[è§‚ç‚¹2];",
  "node_sentiments": {{"B": "positive", "C": "negative"}}
}}
"""

    try:
        response = client.chat.completions.create(
            model=MODEL,
            messages=[
                {"role": "system", "content": "You are a senior public opinion expert." if language == "en" else "ä½ æ˜¯ä¸€ä¸ªé«˜çº§èˆ†æƒ…åˆ†æä¸“å®¶ã€‚"},
                {"role": "user", "content": prompt}
            ],
            response_format={"type": "json_object"}
        )

        final_result = json.loads(response.choices[0].message.content)
        final_result["avg_sentiment"] = avg_sentiment
        return final_result

    except Exception as e:
        print(f"âŒ Reduce é˜¶æ®µå¤±è´¥: {e}")
        return None


# =========================
# 5. ä¸»æµç¨‹
# =========================

def run_analysis(language: str = "zh", keyword: str = None):
    print(f"ğŸš€ å¼€å§‹ AI èˆ†æƒ…åˆ†ææµç¨‹ (è¯­è¨€: {language}, å…³é”®è¯: {keyword or 'å…¨éƒ¨'})...")

    if not os.getenv("OPENAI_API_KEY"):
        print("âŒ æœªæ£€æµ‹åˆ° OPENAI_API_KEY")
        return

    # è¯»å–æ•°æ®
    conn = sqlite3.connect(DB_NAME)
    if keyword:
        df = pd.read_sql_query("SELECT content FROM cleaned_data WHERE keyword = ?", conn, params=(keyword,))
    else:
        df = pd.read_sql_query("SELECT content FROM cleaned_data", conn)
    conn.close()

    if df.empty:
        print(f"âš ï¸ æ•°æ®åº“ä¸­æ²¡æœ‰å¯åˆ†ææ•°æ® (å…³é”®è¯: {keyword or 'å…¨éƒ¨'})")
        return

    # æ¸…æ´—
    df = filter_dirty_data(df)

    # é‡‡æ ·æ§åˆ¶æˆæœ¬
    if len(df) > SAMPLE_SIZE:
        print(f"ğŸ“‰ æ•°æ®é‡è¿‡å¤§ï¼Œé‡‡æ · {SAMPLE_SIZE} æ¡")
        df = df.sample(SAMPLE_SIZE, random_state=42)

    # åˆ†æ‰¹
    batches = []
    current_batch = ""
    current_tokens = 0

    for text in df["content"].tolist():
        tokens = get_token_count(text)

        if current_tokens + tokens > MAX_TOKENS_PER_BATCH:
            batches.append(current_batch.strip())
            current_batch = text
            current_tokens = tokens
        else:
            current_batch += "\n" + text
            current_tokens += tokens

    if current_batch.strip():
        batches.append(current_batch.strip())

    print(f"ğŸ“¦ å…±ç”Ÿæˆ {len(batches)} ä¸ªæ‰¹æ¬¡")

    # Map
    map_results = map_phase(batches, language)
    if not map_results:
        print("âŒ Map é˜¶æ®µæ— ç»“æœ")
        return

    # Reduce
    final_report = reduce_phase(map_results, language, keyword)
    if not final_report:
        return

    # è¾“å‡º
    print("\n" + "=" * 50)
    print("ğŸ“Š AI èˆ†æƒ…åˆ†ææŠ¥å‘Š")
    print("=" * 50)
    print(f"å…³é”®è¯ï¼š{keyword or 'å…¨éƒ¨'}")
    print(f"æ€»ä½“æƒ…æ„Ÿå¾—åˆ†ï¼š{final_report['avg_sentiment']} / 100\n")

    print("ğŸ”¥ ä¸‰å¤§æ ¸å¿ƒäº‰è®®ç‚¹ï¼š")
    for i, p in enumerate(final_report["final_controversies"], 1):
        print(f"{i}. {p}")

    print("\nğŸ“ äººè¯æ‘˜è¦ï¼š")
    print(final_report["human_summary"])
    print("=" * 50)

    # ä¿å­˜ - æŒ‰å…³é”®è¯ä¿å­˜åˆ°ä¸åŒçš„æ–‡ä»¶
    if keyword:
        report_file = f"analysis_report_{keyword}.json"
    else:
        report_file = "analysis_report.json"
    
    with open(report_file, "w", encoding="utf-8") as f:
        json.dump(final_report, f, ensure_ascii=False, indent=2)

    print(f"âœ… å·²ä¿å­˜ {report_file}")
    
    # åŒæ—¶ä¿å­˜åˆ°é€šç”¨æ–‡ä»¶ï¼ˆå‘åå…¼å®¹ï¼‰
    with open("analysis_report.json", "w", encoding="utf-8") as f:
        json.dump(final_report, f, ensure_ascii=False, indent=2)
    
    return final_report


if __name__ == "__main__":
    run_analysis()
