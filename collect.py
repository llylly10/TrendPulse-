import requests
import sqlite3
import time
import html
import traceback
import re
import argparse
from bs4 import BeautifulSoup

# YouTube
from youtube_search import YoutubeSearch  # pip install youtube-search-python
from youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled, NoTranscriptFound

# Twitter
# æ³¨æ„: snscrape å›  Twitter (X) æ”¿ç­–å˜æ›´ç›®å‰å·²å¤±æ•ˆï¼Œæš‚æ³¨é‡Šæ‰
# import snscrape.modules.twitter as sntwitter

# Twitter Selenium å¤‡é€‰æ–¹æ¡ˆ
try:
    from twitter_scraper_selenium import fetch_twitter_selenium
    SELENIUM_AVAILABLE = True
except ImportError:
    SELENIUM_AVAILABLE = False
    print("Warning: Selenium çˆ¬è™«æœªå®‰è£…ï¼Œå°†åªä½¿ç”¨ Nitter é•œåƒç«™")
from data_cleaning import process_data


DB_NAME = "multi_source.db"

# ----------------- 1. åˆå§‹åŒ–æ•°æ®åº“ (ä¼˜åŒ–è¿æ¥ç®¡ç†) -----------------
def init_db():
    with sqlite3.connect(DB_NAME) as conn:
        cur = conn.cursor()

        # é‡‡é›†ä»»åŠ¡è¡¨
        cur.execute("""
        CREATE TABLE IF NOT EXISTS crawl_task (
            task_id INTEGER PRIMARY KEY AUTOINCREMENT,
            source_type TEXT,
            keyword TEXT,
            language TEXT,
            limit_count INTEGER,
            created_at INTEGER
        )
        """)

        # Reddit æ•°æ®
        cur.execute("""
        CREATE TABLE IF NOT EXISTS reddit_submission (
            post_id TEXT PRIMARY KEY,
            task_id INTEGER,
            title TEXT,
            subreddit TEXT,
            score INTEGER,
            num_comments INTEGER,
            created_utc INTEGER,
            is_self INTEGER,
            is_stickied INTEGER,
            url TEXT
        )
        """)

        # YouTube æ•°æ®
        cur.execute("""
        CREATE TABLE IF NOT EXISTS youtube_video (
            video_id TEXT PRIMARY KEY,
            task_id INTEGER,
            title TEXT,
            channel TEXT,
            published_at TEXT,
            view_count INTEGER,
            url TEXT,
            transcript TEXT
        )
        """)

        # Twitter æ•°æ®
        cur.execute("""
        CREATE TABLE IF NOT EXISTS twitter_tweet (
            tweet_id TEXT PRIMARY KEY,
            task_id INTEGER,
            content TEXT,
            username TEXT,
            created_at TEXT,
            retweet_count INTEGER,
            like_count INTEGER,
            url TEXT
        )
        """)
        conn.commit()

# ----------------- 2. åˆ›å»ºé‡‡é›†ä»»åŠ¡ -----------------
def create_task(source_type, keyword, language, limit_count):
    with sqlite3.connect(DB_NAME) as conn:
        cur = conn.cursor()
        cur.execute("""
        INSERT INTO crawl_task (source_type, keyword, language, limit_count, created_at)
        VALUES (?, ?, ?, ?, ?)
        """, (source_type, keyword, language, limit_count, int(time.time())))
        task_id = cur.lastrowid
        conn.commit()
    return task_id

# ----------------- 3. Reddit (å¢å¼ºåçˆ¬ä¼ªè£…) -----------------
def fetch_reddit(keyword, limit=30, language="en"):
    # ä¼ªè£…æˆçœŸå®æµè§ˆå™¨ User-Agentï¼Œé¿å… 429 é”™è¯¯
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }
    url = "https://www.reddit.com/search.json"
    params = {"q": keyword, "limit": limit, "sort": "new", "type": "link"}

    try:
        resp = requests.get(url, headers=headers, params=params, timeout=10)
        
        if resp.status_code == 429:
            print("âŒ Reddit è¿”å› 429 (Too Many Requests). å»ºè®®ä½¿ç”¨å®˜æ–¹ PRAW åº“ã€‚")
            return []
        
        resp.raise_for_status()
        data = resp.json()
        posts = []

        for item in data.get("data", {}).get("children", []):
            p = item["data"]
            posts.append({
                "post_id": p["id"],
                "title": html.unescape(p.get("title", "")),
                "subreddit": p.get("subreddit"),
                "score": p.get("score"),
                "num_comments": p.get("num_comments"),
                "created_utc": p.get("created_utc"),
                "is_self": int(p.get("is_self", False)),
                "is_stickied": int(p.get("stickied", False)),
                "url": "https://www.reddit.com" + p.get("permalink", "")
            })
        return posts
    except requests.exceptions.Timeout:
        print("âŒ Reddit è¯·æ±‚è¶…æ—¶ï¼Œè·³è¿‡ã€‚")
        return []
    except Exception as e:
        print(f"âŒ Reddit æŠ“å–å¤±è´¥: {e}")
        return []

def save_reddit(task_id, posts):
    if not posts: return
    with sqlite3.connect(DB_NAME) as conn:
        cur = conn.cursor()
        for p in posts:
            cur.execute("""
            INSERT OR IGNORE INTO reddit_submission
            (post_id, task_id, title, subreddit, score, num_comments,
             created_utc, is_self, is_stickied, url)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                p["post_id"], task_id, p["title"], p["subreddit"],
                p["score"], p["num_comments"], p["created_utc"],
                p["is_self"], p["is_stickied"], p["url"]
            ))
        conn.commit()

# ----------------- 4. YouTube (å¤„ç†æ•°å­—è½¬æ¢) -----------------
def parse_view_count(view_text):
    """å¤„ç† '1.2M views', '500K views', 'No views' ç­‰æƒ…å†µ"""
    try:
        if not view_text: return 0
        text = view_text.replace("views", "").replace(",", "").strip()
        if "No" in text: return 0
        
        multiplier = 1
        if "K" in text:
            multiplier = 1000
            text = text.replace("K", "")
        elif "M" in text:
            multiplier = 1000000
            text = text.replace("M", "")
        elif "B" in text:
            multiplier = 1000000000
            text = text.replace("B", "")
            
        return int(float(text) * multiplier)
    except:
        return 0

def fetch_youtube(keyword, limit=10, language="en"):
    try:
        results = YoutubeSearch(keyword, max_results=limit).to_dict()
        videos = []
        for r in results:
            videos.append({
                "video_id": r["id"],
                "title": r["title"],
                "channel": r["channel"],
                "published_at": r.get("publish_time", ""),
                "view_count": parse_view_count(r.get("views", "0")), # ä½¿ç”¨æ–°è§£æå‡½æ•°
                "url": "https://www.youtube.com" + r["url_suffix"],
                "transcript": ""
            })
        return videos
    except requests.exceptions.Timeout:
        print("âŒ YouTube è¯·æ±‚è¶…æ—¶ï¼Œè·³è¿‡ã€‚")
        return []
    except Exception as e:
        print(f"âŒ YouTube æœç´¢å¤±è´¥: {e}")
        return []

def fetch_transcripts(videos, lang='en'):
    for v in videos:
        try:
            transcript_obj = YouTubeTranscriptApi.list_transcripts(v["video_id"])
            # ä¼˜å…ˆæ‰¾æ‰‹åŠ¨å­—å¹•ï¼Œæ²¡æœ‰åˆ™æ‰¾è‡ªåŠ¨ç”Ÿæˆçš„
            try:
                t = transcript_obj.find_manually_created_transcript([lang])
            except:
                t = transcript_obj.find_generated_transcript([lang])
            
            transcript_list = t.fetch()
            v["transcript"] = " ".join([x["text"] for x in transcript_list])
            print(f"   âœ… è·å–å­—å¹•æˆåŠŸ: {v['title'][:20]}...")
        except (TranscriptsDisabled, NoTranscriptFound):
            print(f"   âš ï¸ æ— å­—å¹•: {v['title'][:20]}...")
            v["transcript"] = ""
        except Exception as e:
            # print(f"   âŒ å­—å¹•è·å–å‡ºé”™ {v['video_id']}: {e}")
            v["transcript"] = ""
    return videos

def save_youtube(task_id, videos):
    if not videos: return
    with sqlite3.connect(DB_NAME) as conn:
        cur = conn.cursor()
        for v in videos:
            cur.execute("""
            INSERT OR IGNORE INTO youtube_video
            (video_id, task_id, title, channel, published_at, view_count, url, transcript)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                v["video_id"], task_id, v["title"], v["channel"],
                v["published_at"], v["view_count"], v["url"], v["transcript"]
            ))
        conn.commit()

# ----------------- 5. Twitter (ä½¿ç”¨ Nitter é•œåƒç«™) -----------------
def fetch_twitter(keyword, limit=30, language="en"):
    """ä½¿ç”¨ Nitter é•œåƒç«™æŠ“å–æ¨æ–‡å†…å®¹"""
    # å¯ç”¨çš„ Nitter å®ä¾‹åˆ—è¡¨
    instances = [
        "nitter.poast.org",
        "nitter.privacyredirect.com",
        "nitter.tiekoetter.com"
    ]
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }
    
    tweets = []
    
    for instance in instances:
        if len(tweets) >= limit:
            break
            
        url = f"https://{instance}/search"
        params = {"q": keyword, "l": language}
        
        try:
            print(f"   ğŸ” å°è¯•ä» {instance} æŠ“å–...")
            resp = requests.get(url, headers=headers, params=params, timeout=15)
            if resp.status_code != 200:
                print(f"   âš ï¸ {instance} è¿”å›çŠ¶æ€ç  {resp.status_code}")
                continue
                
            soup = BeautifulSoup(resp.text, "html.parser")
            items = soup.select(".timeline-item")
            
            for item in items:
                if len(tweets) >= limit:
                    break
                    
                # æ’é™¤éæ¨æ–‡é¡¹ï¼ˆå¦‚"åŠ è½½æ›´å¤š"ï¼‰
                if "show-more" in item.get("class", []):
                    continue
                
                try:
                    # æå–æ¨æ–‡ ID å’Œ URL
                    tweet_link_el = item.select_one(".tweet-link")
                    if not tweet_link_el:
                        continue
                    tweet_path = tweet_link_el.get("href")  # /username/status/123456#m
                    tweet_id = tweet_path.split("/")[-1].split("#")[0]
                    
                    # æå–å†…å®¹
                    content_el = item.select_one(".tweet-content")
                    content = content_el.get_text(strip=True) if content_el else ""
                    
                    # æå–ç”¨æˆ·å
                    username_el = item.select_one(".username")
                    username = username_el.get_text(strip=True) if username_el else ""
                    
                    # æå–æ—¶é—´
                    date_el = item.select_one(".tweet-date a")
                    created_at = date_el.get("title") if date_el else ""
                    
                    # æå–ç»Ÿè®¡æ•°æ®
                    stats = item.select(".tweet-stats .icon-container")
                    retweet_count = 0
                    like_count = 0
                    for stat in stats:
                        text = stat.get_text(strip=True).replace(",", "")
                        if not text:
                            continue
                        
                        # æ ¹æ®å›¾æ ‡ç±»ååˆ¤æ–­
                        icon = stat.select_one("span")
                        if not icon:
                            continue
                        icon_class = icon.get("class", [])
                        
                        if "icon-retweet" in icon_class:
                            retweet_count = int(text) if text.isdigit() else 0
                        elif "icon-heart" in icon_class:
                            like_count = int(text) if text.isdigit() else 0

                    tweets.append({
                        "tweet_id": tweet_id,
                        "content": content,
                        "username": username,
                        "created_at": created_at,
                        "retweet_count": retweet_count,
                        "like_count": like_count,
                        "url": f"https://twitter.com{tweet_path.split('#')[0]}"
                    })
                except Exception as e:
                    # print(f"   âŒ è§£æå•æ¡æ¨æ–‡å¤±è´¥: {e}")
                    continue
                    
            if tweets:
                print(f"   âœ… ä» {instance} æˆåŠŸè·å– {len(tweets)} æ¡æ¨æ–‡")
                break  # å¦‚æœæŠ“å–åˆ°äº†ï¼Œå°±æš‚æ—¶ä¸å°è¯•å…¶ä»–å®ä¾‹
                
        except requests.exceptions.Timeout:
            print(f"   âŒ è®¿é—® {instance} è¶…æ—¶ï¼Œè·³è¿‡ã€‚")
            continue
        except Exception as e:
            print(f"   âŒ è®¿é—® {instance} å‡ºé”™: {e}")
            continue
            
    # å¦‚æœ Nitter å…¨éƒ¨å¤±è´¥ï¼Œå°è¯• Selenium æ–¹æ¡ˆ
    if not tweets and SELENIUM_AVAILABLE:
        print("   âš ï¸ æ‰€æœ‰ Nitter å®ä¾‹å‡å¤±è´¥ï¼Œåˆ‡æ¢åˆ° Selenium æ–¹æ¡ˆ...")
        try:
            tweets = fetch_twitter_selenium(keyword, limit)
        except Exception as e:
            print(f"   âŒ Selenium æ–¹æ¡ˆä¹Ÿå¤±è´¥äº†: {e}")
    elif not tweets and not SELENIUM_AVAILABLE:
        print("   âš ï¸ Nitter å¤±è´¥ä¸” Selenium æœªå®‰è£…")
        print("   ğŸ’¡ è¿è¡Œ: uv pip install selenium webdriver-manager")
            
    return tweets

def save_twitter(task_id, tweets):
    if not tweets:
        return
    with sqlite3.connect(DB_NAME) as conn:
        cur = conn.cursor()
        for t in tweets:
            cur.execute("""
            INSERT OR IGNORE INTO twitter_tweet
            (tweet_id, task_id, content, username, created_at, retweet_count, like_count, url)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                t["tweet_id"], task_id, t["content"], t["username"],
                t["created_at"], t["retweet_count"], t["like_count"], t["url"]
            ))
        conn.commit()

# ----------------- 6. ç»Ÿä¸€é‡‡é›†å…¥å£ -----------------
def run_collection(keyword, language="en", reddit_limit=30, youtube_limit=30, twitter_limit=30):
    print("--- æ­£åœ¨åˆå§‹åŒ–æ•°æ®åº“ ---")
    init_db()

    # -------- Reddit ----------
    reddit_task_id = create_task("reddit", keyword, language, reddit_limit)
    print(f"\n[Reddit ä»»åŠ¡ {reddit_task_id}] æ­£åœ¨æŠ“å– '{keyword}'...")
    reddit_posts = fetch_reddit(keyword, reddit_limit, language)
    save_reddit(reddit_task_id, reddit_posts)
    print(f"æˆåŠŸä¿å­˜ {len(reddit_posts)} æ¡ Reddit å¸–å­ã€‚")

    # -------- YouTube ----------
    youtube_task_id = create_task("youtube", keyword, language, youtube_limit)
    print(f"\n[YouTube ä»»åŠ¡ {youtube_task_id}] æ­£åœ¨æŠ“å– '{keyword}'...")
    youtube_videos = fetch_youtube(keyword, youtube_limit, language)
    if youtube_videos:
        # è·å–å­—å¹•å¯èƒ½æ¯”è¾ƒæ…¢
        youtube_videos = fetch_transcripts(youtube_videos, language)
        save_youtube(youtube_task_id, youtube_videos)
    print(f"æˆåŠŸä¿å­˜ {len(youtube_videos)} ä¸ª YouTube è§†é¢‘ã€‚")

    # -------- Twitter ----------
    twitter_task_id = create_task("twitter", keyword, language, twitter_limit)
    print(f"\n[Twitter ä»»åŠ¡ {twitter_task_id}] æ­£åœ¨æŠ“å– '{keyword}'...")
    twitter_posts = fetch_twitter(keyword, twitter_limit, language)
    save_twitter(twitter_task_id, twitter_posts)
    print(f"æˆåŠŸä¿å­˜ {len(twitter_posts)} æ¡ Twitter æ¨æ–‡ã€‚")

    # -------- è‡ªåŠ¨æ¸…æ´— ----------
    print("\n--- æ‰€æœ‰é‡‡é›†ä»»åŠ¡å·²å®Œæˆï¼Œå¼€å§‹è‡ªåŠ¨æ¸…æ´—æ•°æ® ---")
    # åªæ¸…æ´—å½“å‰ä»»åŠ¡çš„æ•°æ®
    task_ids = [reddit_task_id, youtube_task_id, twitter_task_id]
    process_data(keyword, task_ids)

# ----------------- ä¸»ç¨‹åº -----------------
def main():
    parser = argparse.ArgumentParser(description="å¤šæºèˆ†æƒ…æ•°æ®é‡‡é›†å·¥å…·")
    parser.add_argument("--keyword", type=str, help="æŸ¥è¯¢å…³é”®è¯")
    parser.add_argument("--language", type=str, choices=["en", "zh"], default="en", help="è¯­è¨€ (en/zh)")
    parser.add_argument("--reddit", type=int, default=30, help="Reddit æŠ“å–é™åˆ¶")
    parser.add_argument("--youtube", type=int, default=30, help="YouTube æŠ“å–é™åˆ¶")
    parser.add_argument("--twitter", type=int, default=30, help="Twitter æŠ“å–é™åˆ¶")
    
    args = parser.parse_args()

    # å¦‚æœæ²¡æœ‰æä¾›å…³é”®è¯ï¼Œåˆ™è¿›å…¥äº¤äº’æ¨¡å¼ï¼ˆæˆ–è€…ä½¿ç”¨é»˜è®¤å€¼ï¼‰
    if not args.keyword:
        print("\nğŸ’¡ æœªæ£€æµ‹åˆ°å‘½ä»¤è¡Œå‚æ•°ï¼Œè¿›å…¥é»˜è®¤é…ç½®æ¨¡å¼...")
        keyword = "DeepSeek"
        language = "en"
        reddit_limit = 30
        youtube_limit = 30
        twitter_limit = 30
    else:
        keyword = args.keyword
        language = args.language
        reddit_limit = args.reddit
        youtube_limit = args.youtube
        twitter_limit = args.twitter
    
    run_collection(keyword, language, reddit_limit, youtube_limit, twitter_limit)

if __name__ == "__main__":
    main()