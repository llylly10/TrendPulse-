from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from apscheduler.schedulers.background import BackgroundScheduler
import sqlite3
import json
import os
import logging
import time
import threading
from datetime import datetime

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(title="Public Opinion Analysis API")

# å…è®¸è·¨åŸŸ (Flutter Web æˆ–å…¶ä»–å‰ç«¯éœ€è¦)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

DB_NAME = "multi_source.db"
REPORT_FILE = "analysis_report.json"

# åˆå§‹åŒ–è°ƒåº¦å™¨
scheduler = BackgroundScheduler()
scheduler.start()

# ä»»åŠ¡çŠ¶æ€è·Ÿè¸ª
task_status = {
    "is_running": False,
    "current_task": None,
    "last_update": 0,
    "progress": "",
}

def get_db_connection():
    if not os.path.exists(DB_NAME):
        # å¦‚æœæ•°æ®åº“ä¸å­˜åœ¨ï¼Œå°è¯•åˆå§‹åŒ–
        try:
            init_db_tables()
        except:
            logger.error(f"Database file {DB_NAME} not found and init failed.")
            return None
    try:
        conn = sqlite3.connect(DB_NAME)
        conn.row_factory = sqlite3.Row
        return conn
    except Exception as e:
        logger.error(f"Error connecting to database: {e}")
        return None

def init_db_tables():
    """åˆå§‹åŒ–æ•°æ®åº“è¡¨ï¼ŒåŒ…æ‹¬æ–°çš„è®¢é˜…å’ŒæŠ¥è­¦è¡¨"""
    with sqlite3.connect(DB_NAME) as conn:
        cur = conn.cursor()
        # ç¡®ä¿åŸæœ‰è¡¨å­˜åœ¨ (ç®€ç•¥)
        
        # è®¢é˜…è¡¨
        cur.execute("""
        CREATE TABLE IF NOT EXISTS subscriptions (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            keyword TEXT NOT NULL,
            language TEXT DEFAULT 'en',
            reddit_limit INTEGER DEFAULT 30,
            youtube_limit INTEGER DEFAULT 30,
            twitter_limit INTEGER DEFAULT 30,
            interval_seconds INTEGER DEFAULT 21600,
            last_run INTEGER DEFAULT 0,
            next_run INTEGER DEFAULT 0,
            execution_count INTEGER DEFAULT 0
        )
        """)
        
        # æŠ¥è­¦è¡¨
        cur.execute("""
        CREATE TABLE IF NOT EXISTS alerts (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            subscription_id INTEGER,
            message TEXT,
            created_at INTEGER,
            is_read INTEGER DEFAULT 0
        )
        """)
        conn.commit()

# ç¡®ä¿å¯åŠ¨æ—¶æ£€æŸ¥è¡¨ç»“æ„
try:
    init_db_tables()
except Exception as e:
    logger.warning(f"DB Init warning: {e}")

def clean_nan(obj):
    """é€’å½’æ¸…ç†å­—å…¸æˆ–åˆ—è¡¨ä¸­çš„ NaN/Inf å€¼"""
    import math
    if isinstance(obj, dict):
        return {k: clean_nan(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [clean_nan(x) for x in obj]
    elif isinstance(obj, float):
        if math.isnan(obj) or math.isinf(obj):
            return 0
        return obj
    return obj

# --- è°ƒåº¦ä»»åŠ¡é€»è¾‘ ---
def scheduled_collection_task(sub_id):
    global task_status
    logger.info(f"Running scheduled task for subscription {sub_id}")
    
    task_status["is_running"] = True
    task_status["current_task"] = f"subscription_{sub_id}"
    task_status["last_update"] = int(time.time())
    task_status["progress"] = "å¼€å§‹æ‰§è¡Œå®šæ—¶ä»»åŠ¡..."
    
    conn = get_db_connection()
    if not conn: 
        task_status["is_running"] = False
        return
    
    try:
        sub = conn.execute("SELECT * FROM subscriptions WHERE id = ?", (sub_id,)).fetchone()
        if not sub: 
            task_status["is_running"] = False
            return
        
        keyword = sub["keyword"]
        language = sub["language"]
        
        # 1. è¿è¡Œé‡‡é›†å’Œåˆ†æ
        from collect import run_collection
        from ai_analysis import run_analysis
        
        task_status["progress"] = f"æ­£åœ¨é‡‡é›†æ•°æ®: {keyword}"
        logger.info(f"Scheduled Collection: {keyword}")
        run_collection(keyword, language, sub["reddit_limit"], sub["youtube_limit"], sub["twitter_limit"])
        
        task_status["progress"] = "æ­£åœ¨è¿›è¡Œ AI åˆ†æ..."
        logger.info("Scheduled Analysis")
        run_analysis(language=language, keyword=keyword)
        
        # 2. æ£€æŸ¥æƒ…æ„Ÿå¾—åˆ†å¹¶æŠ¥è­¦
        task_status["progress"] = "æ£€æŸ¥æƒ…æ„Ÿå¾—åˆ†..."
        if os.path.exists(REPORT_FILE):
            with open(REPORT_FILE, "r", encoding="utf-8") as f:
                report = json.load(f)
                score = report.get("avg_sentiment", 50)
                if score < 30:
                    msg = f"âš ï¸ è´Ÿé¢èˆ†æƒ…æŠ¥è­¦: '{keyword}' æƒ…æ„Ÿå¾—åˆ†ä»… {score:.1f}ï¼"
                    conn.execute("INSERT INTO alerts (subscription_id, message, created_at) VALUES (?, ?, ?)",
                                 (sub_id, msg, int(time.time())))
                    conn.commit()
                    logger.warning(msg)
        
        # 3. æ›´æ–°ä¸‹æ¬¡è¿è¡Œæ—¶é—´å’Œæ‰§è¡Œè®¡æ•°
        now = int(time.time())
        next_run = now + sub["interval_seconds"]
        execution_count = (sub["execution_count"] or 0) + 1
        conn.execute("UPDATE subscriptions SET last_run = ?, next_run = ?, execution_count = ? WHERE id = ?",
                     (now, next_run, execution_count, sub_id))
        conn.commit()
        
        task_status["progress"] = "ä»»åŠ¡å®Œæˆï¼"
        logger.info(f"âœ“ å®šæ—¶ä»»åŠ¡å®Œæˆ: {keyword}")
        
    except Exception as e:
        logger.error(f"Scheduled task failed: {e}")
        task_status["progress"] = f"ä»»åŠ¡å¤±è´¥: {str(e)}"
    finally:
        conn.close()
        task_status["is_running"] = False
        task_status["last_update"] = int(time.time())

def check_subscriptions():
    """æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡æ˜¯å¦æœ‰ä»»åŠ¡éœ€è¦è¿è¡Œ"""
    logger.info("ğŸ” æ£€æŸ¥å®šæ—¶ä»»åŠ¡...")
    conn = get_db_connection()
    if not conn: 
        logger.error("æ•°æ®åº“è¿æ¥å¤±è´¥")
        return
    
    try:
        now = int(time.time())
        # æŸ¥æ‰¾ next_run <= now çš„ä»»åŠ¡
        subs = conn.execute("SELECT * FROM subscriptions WHERE next_run <= ?", (now,)).fetchall()
        
        logger.info(f"æ‰¾åˆ° {len(subs)} ä¸ªå¾…æ‰§è¡Œä»»åŠ¡")
        
        for sub in subs:
            logger.info(f"æ£€æŸ¥è®¢é˜… #{sub['id']}: {sub['keyword']}, next_run={sub['next_run']}, now={now}")
            
            # ç®€å•çš„é˜²é‡å…¥ï¼šå¦‚æœ last_run å¾ˆè¿‘ï¼ˆæ¯”å¦‚1åˆ†é’Ÿå†…ï¼‰ï¼Œè·³è¿‡
            if sub["last_run"] > 0 and now - sub["last_run"] < 60:
                logger.info(f"  è·³è¿‡ï¼ˆæœ€è¿‘åˆšæ‰§è¡Œè¿‡ï¼‰")
                continue
            
            logger.info(f"  âœ“ è§¦å‘ä»»åŠ¡æ‰§è¡Œ: {sub['keyword']}")
            
            # ç›´æ¥åœ¨åå°çº¿ç¨‹ä¸­æ‰§è¡Œä»»åŠ¡ï¼ˆä¸ä½¿ç”¨ schedulerï¼‰
            # è¿™æ ·å¯ä»¥ç«‹å³æ›´æ–° task_statusï¼Œå‰ç«¯å¯ä»¥ç«‹å³çœ‹åˆ°è¿›åº¦
            import threading
            thread = threading.Thread(target=scheduled_collection_task, args=(sub["id"],), daemon=True)
            thread.start()
            
            # æ›´æ–° next_run é¿å…é‡å¤æäº¤
            next_run_temp = now + sub["interval_seconds"]
            conn.execute("UPDATE subscriptions SET next_run = ? WHERE id = ?", (next_run_temp, sub["id"]))
            conn.commit()
            logger.info(f"  ä¸‹æ¬¡è¿è¡Œæ—¶é—´å·²æ›´æ–°: {next_run_temp}")
            
    except Exception as e:
        logger.error(f"Subscription check failed: {e}")
    finally:
        conn.close()

# æ·»åŠ å®šæ—¶æ£€æŸ¥å™¨
scheduler.add_job(check_subscriptions, 'interval', minutes=1)


@app.get("/")
async def root():
    return {"status": "ok", "message": "Public Opinion Analysis API is running"}

@app.get("/api/dashboard")
async def get_dashboard(keyword: str = None):
    # 1. æ£€æŸ¥æ•°æ®åº“æ˜¯å¦æœ‰æ•°æ®
    conn = get_db_connection()
    if not conn:
        raise HTTPException(status_code=500, detail="Database connection failed")
    
    try:
        cursor = conn.cursor()
        
        # å¦‚æœæ²¡æœ‰æŒ‡å®šå…³é”®è¯ï¼Œè·å–æœ€æ–°çš„å…³é”®è¯
        if not keyword:
            cursor.execute("SELECT keyword FROM cleaned_data WHERE keyword != 'unknown' ORDER BY rowid DESC LIMIT 1")
            result = cursor.fetchone()
            if result:
                keyword = result["keyword"]
                logger.info(f"Dashboard: ä½¿ç”¨æœ€æ–°å…³é”®è¯ '{keyword}'")
        
        # æŸ¥è¯¢æŒ‡å®šå…³é”®è¯çš„æ•°æ®é‡
        if keyword:
            cursor.execute("SELECT COUNT(*) as total FROM cleaned_data WHERE keyword = ?", (keyword,))
        else:
            cursor.execute("SELECT COUNT(*) as total FROM cleaned_data")
        
        total_count = cursor.fetchone()["total"]
        
        # å¦‚æœæ²¡æœ‰æ•°æ®ï¼Œè¿”å›ç©ºçŠ¶æ€
        if total_count == 0:
            conn.close()
            return {
                "heat_index": 0.0,
                "total_posts": 0,
                "sentiment": {
                    "score": 50.0,
                    "label": "æš‚æ— æ•°æ®"
                },
                "key_points": [],
                "summary": "",
                "mermaid_graph": "",
                "node_sentiments": {},
                "keyword": keyword or ""
            }
        
        # 2. è¯»å– AI åˆ†ææŠ¥å‘Š - æŒ‰å…³é”®è¯è¯»å–å¯¹åº”çš„æŠ¥å‘Šæ–‡ä»¶
        report = {}
        if keyword:
            report_file = f"analysis_report_{keyword}.json"
        else:
            report_file = REPORT_FILE
        
        if os.path.exists(report_file):
            try:
                with open(report_file, "r", encoding="utf-8") as f:
                    report = json.load(f)
                logger.info(f"Dashboard: è¯»å–æŠ¥å‘Šæ–‡ä»¶ {report_file}")
            except Exception as e:
                logger.error(f"Error reading report file {report_file}: {e}")
        else:
            logger.warning(f"Dashboard: æŠ¥å‘Šæ–‡ä»¶ {report_file} ä¸å­˜åœ¨")
        
        # å¦‚æœæŠ¥å‘Šä¸ºç©ºï¼Œè¿”å›åŸºç¡€æ•°æ®
        if not report:
            conn.close()
            return {
                "heat_index": 0.0,
                "total_posts": total_count,
                "sentiment": {
                    "score": 50.0,
                    "label": "åˆ†æä¸­"
                },
                "key_points": [],
                "summary": "æ•°æ®åˆ†æä¸­ï¼Œè¯·ç¨å...",
                "mermaid_graph": ""
            }
        
        # è·å–äº’åŠ¨æ•° (è§£æ engagement JSON)
        if keyword:
            cursor.execute("SELECT engagement FROM cleaned_data WHERE keyword = ?", (keyword,))
        else:
            cursor.execute("SELECT engagement FROM cleaned_data")
        rows = cursor.fetchall()
        total_engagement = 0
        import math
        def safe_add(current, val):
            try:
                v = float(val) if val is not None else 0
                if math.isnan(v) or math.isinf(v):
                    return current
                return current + v
            except:
                return current

        for row in rows:
            try:
                eng_str = row["engagement"]
                if eng_str:
                    eng = json.loads(eng_str)
                    total_engagement = safe_add(total_engagement, eng.get("score"))
                    total_engagement = safe_add(total_engagement, eng.get("view_count"))
                    total_engagement = safe_add(total_engagement, eng.get("retweet_count"))
                    total_engagement = safe_add(total_engagement, eng.get("like_count"))
                    total_engagement = safe_add(total_engagement, eng.get("num_comments"))
            except Exception as e:
                logger.warning(f"Error parsing engagement JSON: {e}")
                continue
        
        conn.close()
    except Exception as e:
        logger.error(f"Error querying database: {e}")
        if conn:
            conn.close()
        raise HTTPException(status_code=500, detail="Database query failed")
    
    # è®¡ç®—çƒ­åº¦æŒ‡æ ‡ (ç®€å•åŠ æƒ)
    heat_index = total_count + (total_engagement / 10.0 if total_engagement else 0)
    
    # ç¡®ä¿ heat_index ä¸æ˜¯ NaN
    if math.isnan(heat_index) or math.isinf(heat_index):
        heat_index = 0
    
    return clean_nan({
        "heat_index": float(heat_index),
        "total_posts": int(total_count),
        "sentiment": {
            "score": float(report.get("avg_sentiment", 50)),
            "label": "æ­£é¢" if report.get("avg_sentiment", 50) > 60 else ("è´Ÿé¢" if report.get("avg_sentiment", 50) < 40 else "ä¸­æ€§")
        },
        "key_points": report.get("final_controversies", []),
        "summary": report.get("human_summary", "æš‚æ— æ‘˜è¦"),
        "mermaid_graph": report.get("mermaid_graph", ""),
        "node_sentiments": report.get("node_sentiments", {}),
        "keyword": keyword or ""
    })

@app.get("/api/source-data")
async def get_source_data(keyword: str = None):
    conn = get_db_connection()
    if not conn:
        raise HTTPException(status_code=500, detail="Database connection failed")
    
    try:
        cursor = conn.cursor()
        
        # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='cleaned_data'")
        if not cursor.fetchone():
            conn.close()
            return []
        
        # å¦‚æœæ²¡æœ‰æŒ‡å®šå…³é”®è¯ï¼Œè·å–æœ€æ–°çš„å…³é”®è¯
        if not keyword:
            cursor.execute("SELECT keyword FROM cleaned_data WHERE keyword != 'unknown' ORDER BY rowid DESC LIMIT 1")
            result = cursor.fetchone()
            if result:
                keyword = result["keyword"]
        
        # æŒ‰å…³é”®è¯æŸ¥è¯¢
        if keyword:
            cursor.execute("SELECT platform, content, author, timestamp, engagement, url, keyword FROM cleaned_data WHERE keyword = ? ORDER BY timestamp DESC", (keyword,))
        else:
            cursor.execute("SELECT platform, content, author, timestamp, engagement, url, keyword FROM cleaned_data ORDER BY timestamp DESC")
        
        rows = cursor.fetchall()
        
        result = []
        for row in rows:
            item = dict(row)
            try:
                if item["engagement"]:
                    item["engagement"] = json.loads(item["engagement"])
                else:
                    item["engagement"] = {}
            except:
                item["engagement"] = {}
            result.append(item)
        
        conn.close()
        return clean_nan(result)
    except Exception as e:
        logger.error(f"Error querying database: {e}")
        if conn:
            conn.close()
        return []

@app.post("/api/collect")
async def collect_data(params: dict, background_tasks: BackgroundTasks):
    global task_status
    
    keyword = params.get("keyword", "DeepSeek")
    language = params.get("language", "en")
    reddit_limit = params.get("reddit_limit", 30)
    youtube_limit = params.get("youtube_limit", 30)
    twitter_limit = params.get("twitter_limit", 30)
    
    def run_pipeline():
        global task_status
        task_status["is_running"] = True
        task_status["current_task"] = f"manual_{keyword}"
        task_status["last_update"] = int(time.time())
        
        try:
            from collect import run_collection
            from ai_analysis import run_analysis
            
            task_status["progress"] = f"æ­£åœ¨é‡‡é›†æ•°æ®: {keyword}"
            logger.info(f"Starting collection for: {keyword}")
            run_collection(keyword, language, reddit_limit, youtube_limit, twitter_limit)
            
            task_status["progress"] = "æ­£åœ¨è¿›è¡Œ AI åˆ†æ..."
            logger.info("Starting AI analysis")
            run_analysis(language=language, keyword=keyword)
            
            task_status["progress"] = "ä»»åŠ¡å®Œæˆï¼"
            logger.info("Pipeline completed successfully")
        except Exception as e:
            logger.error(f"Pipeline failed: {e}")
            task_status["progress"] = f"ä»»åŠ¡å¤±è´¥: {str(e)}"
        finally:
            task_status["is_running"] = False
            task_status["last_update"] = int(time.time())

    background_tasks.add_task(run_pipeline)
    return {"status": "accepted", "message": "Collection and analysis started in background"}

# è·å–ä»»åŠ¡çŠ¶æ€
@app.get("/api/task-status")
async def get_task_status():
    return task_status


# --- è®¢é˜…ç›¸å…³ API ---

@app.get("/api/subscriptions")
async def get_subscriptions():
    conn = get_db_connection()
    if not conn: raise HTTPException(status_code=500)
    try:
        subs = conn.execute("SELECT * FROM subscriptions ORDER BY id DESC").fetchall()
        return [dict(row) for row in subs]
    finally:
        conn.close()

@app.post("/api/subscriptions")
async def create_subscription(params: dict):
    conn = get_db_connection()
    if not conn: raise HTTPException(status_code=500)
    try:
        keyword = params.get("keyword")
        if not keyword: raise HTTPException(status_code=400, detail="Keyword required")
        
        # è®¡ç®—é—´éš”ç§’æ•°
        interval_seconds = params.get("interval_seconds", 21600)  # é»˜è®¤ 6 å°æ—¶
        
        conn.execute("""
            INSERT INTO subscriptions (keyword, language, reddit_limit, youtube_limit, twitter_limit, interval_seconds, next_run)
            VALUES (?, ?, ?, ?, ?, ?, ?)
        """, (
            keyword,
            params.get("language", "en"),
            params.get("reddit_limit", 30),
            params.get("youtube_limit", 30),
            params.get("twitter_limit", 30),
            interval_seconds,
            int(time.time()) # ç«‹å³è¿è¡Œä¸€æ¬¡? æˆ–è€…ç¨å. è¿™é‡Œè®¾ä¸ºå½“å‰æ—¶é—´æ„å‘³ç€ä¸‹æ¬¡æ£€æŸ¥ä¼šç«‹å³è§¦å‘
        ))
        conn.commit()
        return {"status": "ok"}
    finally:
        conn.close()

@app.delete("/api/subscriptions/{id}")
async def delete_subscription(id: int):
    conn = get_db_connection()
    if not conn: raise HTTPException(status_code=500)
    try:
        conn.execute("DELETE FROM subscriptions WHERE id = ?", (id,))
        conn.commit()
        return {"status": "ok"}
    finally:
        conn.close()

@app.get("/api/alerts")
async def get_alerts():
    conn = get_db_connection()
    if not conn: raise HTTPException(status_code=500)
    try:
        alerts = conn.execute("SELECT * FROM alerts ORDER BY created_at DESC LIMIT 50").fetchall()
        return [dict(row) for row in alerts]
    finally:
        conn.close()

@app.post("/api/clear-data")
async def clear_data():
    """æ¸…ç©ºæ‰€æœ‰é‡‡é›†æ•°æ®å’Œåˆ†ææŠ¥å‘Š"""
    try:
        # 1. åˆ é™¤æŠ¥å‘Šæ–‡ä»¶
        if os.path.exists(REPORT_FILE):
            os.remove(REPORT_FILE)
            logger.info(f"Deleted {REPORT_FILE}")
        
        # 2. æ¸…ç©ºæ•°æ®åº“è¡¨
        conn = get_db_connection()
        if conn:
            try:
                conn.execute("DELETE FROM cleaned_data")
                conn.commit()
                logger.info("Cleared cleaned_data table")
            except Exception as e:
                logger.warning(f"Error clearing cleaned_data: {e}")
            finally:
                conn.close()
        
        return {"status": "ok", "message": "æ‰€æœ‰æ•°æ®å·²æ¸…ç©º"}
    except Exception as e:
        logger.error(f"Error clearing data: {e}")
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8888)

