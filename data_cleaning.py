import sqlite3
import re
import html
import json
from datetime import datetime
import pandas as pd

DB_NAME = "multi_source.db"

def clean_text(text):
    if not text:
        return ""
    
    # 1. è§£ç  HTML å®žä½“ (å¦‚ &amp; -> &)
    text = html.unescape(text)
    
    # 2. åŽ»é™¤ URL é“¾æŽ¥
    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)
    
    # 3. ä¿ç•™ä¸­æ–‡å­—ç¬¦ã€è‹±æ–‡å­—ç¬¦ã€æ•°å­—å’ŒåŸºæœ¬æ ‡ç‚¹ï¼ŒåŽ»é™¤å…¶ä»–æ‚è´¨
    # \u4e00-\u9fa5 æ˜¯ä¸­æ–‡èŒƒå›´
    text = re.sub(r'[^\w\s\u4e00-\u9fa5,.!?ï¼Œã€‚ï¼ï¼Ÿ]', '', text)
    
    # 4. è§„èŒƒåŒ–ç©ºç™½å­—ç¬¦
    text = re.sub(r'\s+', ' ', text).strip()
    
    return text

def normalize_time(val):
    """å°†å„ç§æ—¶é—´æ ¼å¼ç»Ÿä¸€ä¸º ISO-8601 å­—ç¬¦ä¸²"""
    if not val:
        return None
    
    try:
        # å¦‚æžœæ˜¯ Reddit çš„ Unix æ—¶é—´æˆ³ (int/float)
        if isinstance(val, (int, float)):
            return datetime.fromtimestamp(val).isoformat()
        
        # å¦‚æžœæ˜¯å­—ç¬¦ä¸²æ ¼å¼
        val_str = str(val).strip()
        
        # å°è¯•è§£æžå¸¸è§çš„ ISO æ ¼å¼æˆ– Twitter/YouTube æ ¼å¼
        # pandas çš„ to_datetime éžå¸¸å¼ºå¤§ï¼Œå¯ä»¥å¤„ç†å¤§å¤šæ•°æƒ…å†µ
        dt = pd.to_datetime(val_str, errors='coerce')
        if pd.notnull(dt):
            return dt.isoformat()
            
    except Exception as e:
        print(f"âš ï¸ æ—¶é—´è½¬æ¢å¤±è´¥: {val} -> {e}")
    
    return str(val)

def process_data(keyword="unknown", task_ids=None):
    print(f"ðŸš€ å¼€å§‹æ•°æ®æ¸…æ´—æµç¨‹ (å…³é”®è¯: {keyword})...")
    
    conn = sqlite3.connect(DB_NAME)
    
    # å¦‚æžœæŒ‡å®šäº† task_idsï¼Œåªå¤„ç†è¿™äº›ä»»åŠ¡çš„æ•°æ®
    if task_ids:
        task_filter = f"WHERE task_id IN ({','.join(map(str, task_ids))})"
    else:
        task_filter = ""
    
    # 1. è¯»å– Reddit æ•°æ®
    print("ðŸ“¥ è¯»å– Reddit æ•°æ®...")
    reddit_query = f"SELECT post_id, title, subreddit, score, created_utc, url FROM reddit_submission {task_filter}"
    reddit_df = pd.read_sql_query(reddit_query, conn)
    reddit_df = reddit_df.rename(columns={
        'post_id': 'raw_id',
        'title': 'content',
        'subreddit': 'author',
        'created_utc': 'raw_time'
    })
    reddit_df['platform'] = 'reddit'
    reddit_df['engagement'] = reddit_df['score'].apply(lambda x: json.dumps({'score': x}))

    # 2. è¯»å– YouTube æ•°æ®
    print("ðŸ“¥ è¯»å– YouTube æ•°æ®...")
    youtube_query = f"SELECT video_id, title, channel, published_at, view_count, url FROM youtube_video {task_filter}"
    youtube_df = pd.read_sql_query(youtube_query, conn)
    youtube_df = youtube_df.rename(columns={
        'video_id': 'raw_id',
        'title': 'content',
        'channel': 'author',
        'published_at': 'raw_time'
    })
    youtube_df['platform'] = 'youtube'
    youtube_df['engagement'] = youtube_df['view_count'].apply(lambda x: json.dumps({'view_count': x}))

    # 3. è¯»å– Twitter æ•°æ®
    print("ðŸ“¥ è¯»å– Twitter æ•°æ®...")
    twitter_query = f"SELECT tweet_id, content, username, created_at, retweet_count, like_count, url FROM twitter_tweet {task_filter}"
    twitter_df = pd.read_sql_query(twitter_query, conn)
    twitter_df = twitter_df.rename(columns={
        'tweet_id': 'raw_id',
        'username': 'author',
        'created_at': 'raw_time'
    })
    twitter_df['platform'] = 'twitter'
    twitter_df['engagement'] = twitter_df.apply(lambda r: json.dumps({'retweet_count': r['retweet_count'], 'like_count': r['like_count']}), axis=1)

    # åˆå¹¶æ‰€æœ‰æ•°æ®
    print("ðŸ”„ åˆå¹¶æ•°æ®å¹¶è¿›è¡Œæ¸…æ´—...")
    all_data = pd.concat([reddit_df, youtube_df, twitter_df], ignore_index=True)
    
    if all_data.empty:
        print("âš ï¸ æ²¡æœ‰æ•°æ®éœ€è¦æ¸…æ´—")
        conn.close()
        return

    # æ‰§è¡Œæ¸…æ´—é€»è¾‘
    all_data['content'] = all_data['content'].apply(clean_text)
    all_data['timestamp'] = all_data['raw_time'].apply(normalize_time)
    
    # åŽ»é‡
    initial_count = len(all_data)
    all_data = all_data.drop_duplicates(subset=['platform', 'raw_id'])
    print(f"ðŸ§¹ åŽ»é‡å®Œæˆ: {initial_count} -> {len(all_data)}")

    # å‡†å¤‡å­˜å…¥æ•°æ®åº“çš„æœ€ç»ˆå­—æ®µ
    final_df = all_data[['platform', 'raw_id', 'content', 'author', 'timestamp', 'engagement', 'url']]
    
    # æ·»åŠ å…³é”®è¯å­—æ®µ
    final_df['keyword'] = keyword

    # å­˜å…¥æ•°æ®åº“
    print(f"ðŸ’¾ æ­£åœ¨å°†æ¸…æ´—åŽçš„æ•°æ®å­˜å…¥ 'cleaned_data' è¡¨ (å…³é”®è¯: {keyword})...")
    final_df.to_sql('cleaned_data', conn, if_exists='append', index=False)
    
    conn.close()
    print("âœ… æ•°æ®æ¸…æ´—å®Œæˆï¼")

if __name__ == "__main__":
    import sys
    keyword = sys.argv[1] if len(sys.argv) > 1 else "unknown"
    process_data(keyword)
